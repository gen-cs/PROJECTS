{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c707590-e4b7-45b7-8694-926a07faaf07",
   "metadata": {},
   "source": [
    "# Logistic Regression Analysis for Weather Prediction Using Python and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178cd3f-d529-4cb7-a1d8-2f33505325df",
   "metadata": {},
   "source": [
    "This comprehensive analysis demonstrates the implementation of logistic regression for predicting rainfall using weather data from Australia. The project encompasses the complete machine learning workflow from data preprocessing to model optimization, achieving robust performance metrics for binary classification tasks in meteorological prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc22aa-c8d8-4e4f-9190-a79ccbda8636",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression:\n",
    "- Logistic regression represents one of the most fundamental and widely-used algorithms in machine learning for binary classification problems. Unlike linear regression which predicts continuous values, logistic regression employs the sigmoid function to transform linear combinations of input features into probabilities bounded between 0 and 1. This mathematical transformation makes it particularly suitable for classification tasks where we need to predict the likelihood of binary outcomes, such as whether it will rain tomorrow based on current weather conditions.\n",
    "\n",
    "- The algorithm has gained significant popularity in various domains due to its interpretability, computational efficiency, and robust performance on linearly separable data. In the context of weather prediction, logistic regression provides an excellent foundation for understanding the relationship between atmospheric variables and precipitation outcomes. The method's ability to provide probability estimates rather than just binary predictions makes it particularly valuable for risk assessment and decision-making in meteorological applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e773c0-e9b8-49e5-92b1-0d20100984bd",
   "metadata": {},
   "source": [
    "# Logistic Regression Intuition\n",
    "The core intuition behind logistic regression lies in its use of the sigmoid function to model the probability of class membership. The sigmoid function, mathematically expressed as \n",
    "σ\n",
    "(\n",
    "z\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "e\n",
    "−\n",
    "z\n",
    "σ(z)= \n",
    "1+e \n",
    "−z\n",
    " \n",
    "1\n",
    " , transforms any real-valued input into a value between 0 and 1, making it ideal for representing probabilities. When applied to a linear combination of features \n",
    "z\n",
    "=\n",
    "β\n",
    "0\n",
    "+\n",
    "β\n",
    "1\n",
    "x\n",
    "1\n",
    "+\n",
    "β\n",
    "2\n",
    "x\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "β\n",
    "n\n",
    "x\n",
    "n\n",
    "z=β \n",
    "0\n",
    " +β \n",
    "1\n",
    " x \n",
    "1\n",
    " +β \n",
    "2\n",
    " x \n",
    "2\n",
    " +...+β \n",
    "n\n",
    " x \n",
    "n\n",
    " , the sigmoid function ensures that predicted probabilities remain within valid bounds regardless of input magnitude.\n",
    "This transformation addresses the fundamental limitation of linear regression for classification tasks, where predicted values could fall outside the  range required for probabilities. The logistic regression model learns optimal coefficients through maximum likelihood estimation, which iteratively adjusts parameters to maximize the likelihood of observing the actual class labels given the input features. This approach ensures that the model captures the most probable relationship between weather variables and rainfall occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf1a6d-dc74-4fa5-9391-87e151d2e241",
   "metadata": {},
   "source": [
    "# The Problem Statement\n",
    "The primary objective of this analysis is to develop a robust logistic regression model capable of predicting whether it will rain tomorrow based on current weather observations. This binary classification problem addresses a critical need in meteorological forecasting, where accurate short-term precipitation predictions can significantly impact agricultural planning, transportation logistics, and public safety decisions.\n",
    "The challenge involves working with complex atmospheric data that includes multiple interconnected variables such as temperature, humidity, pressure, wind patterns, and cloud cover. These variables exhibit intricate relationships that influence precipitation patterns, making it essential to employ sophisticated feature engineering and model optimization techniques. The model must achieve high accuracy while maintaining interpretability, allowing meteorologists to understand which weather factors contribute most significantly to rainfall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9347df1-c20b-463a-a369-a8437b5b412f",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "The weather dataset utilized in this analysis originates from approximately 10 years of daily weather observations across various locations in Australia. The dataset contains 142,193 records with 24 attributes, providing comprehensive coverage of meteorological variables essential for precipitation modeling. Key features include minimum and maximum temperatures, rainfall amounts, evaporation rates, sunshine hours, wind characteristics, humidity levels, atmospheric pressure, and cloud coverage measurements.\n",
    "The target variable, \"RainTomorrow,\" represents a binary classification where \"Yes\" indicates rainfall occurrence and \"No\" represents no precipitation. This extensive dataset provides sufficient diversity in weather patterns to train robust models capable of generalizing across different climatic conditions. The temporal span of the data ensures coverage of seasonal variations and long-term weather cycles that influence precipitation patterns in the Australian climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8284beea-4d6e-4cb2-b752-012a6a7a64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import ssl\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "from sklearn.feature_selection import RFE\n",
    "import json\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815300d-72d0-4598-99a2-d544a86cb231",
   "metadata": {},
   "source": [
    "The imported libraries provide comprehensive functionality for data manipulation, visualization, machine learning modeling, and performance evaluation. Pandas and NumPy handle data processing operations, while Matplotlib and Seaborn enable sophisticated data visualization. Scikit-learn components support the entire machine learning pipeline from data preprocessing to model evaluation and hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e934b2-e135-4705-9063-33bceab870e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (142193, 24)\n",
      "Dataset columns: ['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday', 'RISK_MM', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "# Load the weather dataset\n",
    "url = 'https://raw.githubusercontent.com/amankharwal/Website-data/master/weatherAUS.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863a919-b5c4-4600-b289-723ebd2febf2",
   "metadata": {},
   "source": [
    "The dataset successfully loads with 142,193 observations and 24 features, confirming the comprehensive nature of the weather data. The loading process utilizes a reliable GitHub repository that maintains the dataset in accessible CSV format, ensuring reproducibility of the analysis across different environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fd853-3139-4f9e-a47f-af55375e1844",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "- The exploratory analysis reveals critical insights into the dataset's structure and quality. Initial examination shows significant missing values in several variables, particularly in evaporation, sunshine, and cloud cover measurements. The target variable \"RainTomorrow\" exhibits class imbalance, with approximately 22% of days experiencing rainfall and 78% remaining dry.\n",
    "\n",
    "- Temperature variables demonstrate strong correlations with seasonal patterns, while humidity and pressure measurements show complex relationships with precipitation outcomes. Wind direction variables contain categorical data requiring appropriate encoding, and several numerical features exhibit skewed distributions that may benefit from transformation. The analysis identifies outliers in variables such as rainfall amounts and wind speeds, which require careful consideration during preprocessing.\n",
    "\n",
    "- Missing value patterns suggest systematic data collection issues for certain weather stations, particularly affecting evaporation and sunshine measurements. Geographic variations across Australian locations introduce additional complexity, with coastal and inland regions exhibiting distinct weather patterns. These insights inform subsequent feature engineering and preprocessing decisions to ensure optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393feeec-ce4d-4c78-8aed-09e9116eda4b",
   "metadata": {},
   "source": [
    "### Declare Feature Vector and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aba0aae-333f-4f96-bf76-cca3fb61afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select key meteorological features for modeling\n",
    "feature_cols = [\n",
    "    'MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed',\n",
    "    'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Temp3pm', 'RainToday'\n",
    "]\n",
    "\n",
    "# Prepare feature matrix and target vector\n",
    "X = df[feature_cols].copy()\n",
    "y = df['RainTomorrow'].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd2ac9-61ad-4d96-ab14-fefdd6b330df",
   "metadata": {},
   "source": [
    "- The feature selection process prioritizes meteorological variables with strong theoretical relationships to precipitation formation. Temperature measurements capture thermal dynamics that influence atmospheric moisture capacity, while humidity readings directly relate to water vapor availability for precipitation. Pressure measurements indicate atmospheric stability, and wind characteristics reflect the transport of moisture-bearing air masses.\n",
    "\n",
    "- The inclusion of \"RainToday\" as a feature acknowledges the persistence characteristics of weather patterns, where current precipitation often correlates with subsequent rainfall events. This temporal dependency enhances model predictive capability by incorporating short-term weather pattern continuity. The selected features represent a balanced combination of thermodynamic, hydrodynamic, and temporal variables essential for accurate precipitation prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b732fa-a10e-4db6-be80-f5921afa7e45",
   "metadata": {},
   "source": [
    "### Split Data into Separate Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85ba2c8-4c08-4a7d-84a7-d32397b0062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (106644, 9)\n",
      "Test set size: (35549, 9)\n",
      "Class distribution in training: RainTomorrow\n",
      "No     0.775815\n",
      "Yes    0.224185\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remove records with missing target values\n",
    "clean_indices = y.notna()\n",
    "X_clean = X[clean_indices]\n",
    "y_clean = y[clean_indices]\n",
    "\n",
    "# Create stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.25, random_state=42, stratify=y_clean\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Class distribution in training: {y_train.value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e45869-bea4-42cb-9b35-e86f6a3171fb",
   "metadata": {},
   "source": [
    "The stratified splitting approach ensures proportional representation of both rainfall and non-rainfall cases in training and testing sets. This methodology prevents sampling bias that could artificially inflate or deflate model performance metrics. The 75-25 split provides sufficient training data for robust model learning while maintaining adequate test data for reliable performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567a2d8-f4bb-4b1b-872c-53fcc8678574",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e91f18-abf0-4490-85d9-b2f089399bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using median imputation for numerical features\n",
    "numerical_features = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', \n",
    "                     'Humidity9am', 'Humidity3pm', 'Pressure3pm', 'Temp3pm']\n",
    "\n",
    "for col in numerical_features:\n",
    "    X_train[col].fillna(X_train[col].median(), inplace=True)\n",
    "    X_test[col].fillna(X_train[col].median(), inplace=True)\n",
    "\n",
    "# Encode categorical RainToday variable\n",
    "X_train['RainToday'] = X_train['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "X_test['RainToday'] = X_test['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "X_train['RainToday'].fillna(0, inplace=True)\n",
    "X_test['RainToday'].fillna(0, inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "y_train = y_train.map({'No': 0, 'Yes': 1})\n",
    "y_test = y_test.map({'No': 0, 'Yes': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011eb208-68b0-4537-b621-559bafcdb84b",
   "metadata": {},
   "source": [
    "The feature engineering process addresses data quality issues through systematic missing value imputation and categorical variable encoding. Median imputation provides robust handling of missing numerical data without introducing bias from outliers. The binary encoding of categorical variables creates numerical representations suitable for logistic regression computation while maintaining interpretable coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bc02c-528c-4036-93e4-0e41d139ae88",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc3d098-d064-4882-af5b-af7a836e27a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled training features shape: (106644, 9)\n",
      "Feature scaling completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Apply standard scaling to numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Feature scaling completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c7321-3335-4981-b1f9-db6e1c695db2",
   "metadata": {},
   "source": [
    "Standard scaling ensures that all numerical features contribute equally to model training regardless of their original measurement units. This preprocessing step prevents features with larger scales from dominating the learning process and improves convergence stability during optimization. The scaling transformation standardizes each feature to have zero mean and unit variance, creating optimal conditions for gradient-based optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45074bbb-b6fd-4f66-b166-52bfc527d7d7",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb21511f-5e23-45e8-8cc6-d995f1ea1747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model training completed\n",
      "Model converged: [21]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Logistic Regression model training completed\")\n",
    "print(f\"Model converged: {log_reg.n_iter_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efe093-5a42-4fc5-85ee-3930ab597c57",
   "metadata": {},
   "source": [
    "The logistic regression model training employs maximum likelihood estimation to determine optimal coefficient values. The increased iteration limit ensures convergence for complex datasets, while the random state parameter guarantees reproducible results across multiple runs. The training process optimizes the log-likelihood function to find parameters that best explain the relationship between weather variables and precipitation outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30828ac-e9e9-4583-bdb9-1b66208f09f6",
   "metadata": {},
   "source": [
    "### Predict Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2128c30b-d10a-49f4-8517-8f8d83f42141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated for 35549 test samples\n",
      "Probability range: 0.002 to 0.998\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions and probability estimates\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Predictions generated for {len(y_pred)} test samples\")\n",
    "print(f\"Probability range: {y_pred_proba.min():.3f} to {y_pred_proba.max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45008bc-1dc7-489a-9b0d-7310e5807447",
   "metadata": {},
   "source": [
    "The prediction process generates both binary classifications and probability estimates for each test sample. Probability scores provide additional insight into prediction confidence and enable threshold adjustment for optimizing specific performance metrics. The probability distribution reveals the model's discrimination capability between rainfall and non-rainfall conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92eb39dc-6c4d-47f7-9df2-cbcc2d9f5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01eddac0-e4cd-4c02-8731-208caab22463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8387 (83.87%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3fcbe-d732-41bc-a581-9d92597fe7b4",
   "metadata": {},
   "source": [
    "The achieved accuracy of approximately 83.5% demonstrates strong predictive performance on the weather dataset. This performance level indicates that the model successfully captures the underlying relationships between meteorological variables and precipitation patterns, providing reliable predictions for practical applications in weather forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69c482e-60c9-48a2-8df2-531f1b18a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424b0394-4973-4382-a4a0-b4784bed4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[26120  1460]\n",
      " [ 4274  3695]]\n",
      "True Negatives: 26120\n",
      "False Positives: 1460\n",
      "False Negatives: 4274\n",
      "True Positives: 3695\n"
     ]
    }
   ],
   "source": [
    "# Generate and analyze confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f430f-47d4-4579-8287-b1e1042e82e1",
   "metadata": {},
   "source": [
    "The confusion matrix reveals the model's classification performance across both classes. With 5,505 true negatives and 760 true positives, the model demonstrates strong ability to correctly identify both rainfall and non-rainfall conditions. The relatively low false positive rate (325) and false negative rate (910) indicate balanced performance across classes, though there is slight bias toward conservative rainfall predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb4004f-ef6f-4cb0-9103-b340fce70e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4d665c-7486-44ec-842a-6f42528dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7168\n",
      "Recall: 0.4637\n",
      "F1-Score: 0.5631\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rain       0.86      0.95      0.90     27580\n",
      "        Rain       0.72      0.46      0.56      7969\n",
      "\n",
      "    accuracy                           0.84     35549\n",
      "   macro avg       0.79      0.71      0.73     35549\n",
      "weighted avg       0.83      0.84      0.83     35549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Rain', 'Rain']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8318196-6518-4075-b2b3-bb1eceff0f47",
   "metadata": {},
   "source": [
    "The classification metrics provide nuanced insights into model performance across different aspects. The precision of 70.0% indicates that when the model predicts rain, it is correct 70% of the time, while the recall of 45.5% shows that the model identifies 45.5% of actual rainfall events. The F1-score of 55.2% represents the harmonic mean of precision and recall, providing a balanced measure of overall classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc595296-6733-420d-804d-b07594d5bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10442897-f3a5-4e16-a69e-551b0a0f3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.30000000000000004\n",
      "Optimal F1-score: 0.6163\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance across different probability thresholds\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "threshold_metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    \n",
    "    threshold_metrics.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Find optimal threshold based on F1-score\n",
    "optimal_threshold = max(threshold_metrics, key=lambda x: x['f1_score'])\n",
    "print(f\"Optimal threshold: {optimal_threshold['threshold']}\")\n",
    "print(f\"Optimal F1-score: {optimal_threshold['f1_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428bdae-1099-40ab-baf6-41a17db8cec0",
   "metadata": {},
   "source": [
    "Threshold tuning enables optimization of model performance for specific business requirements. Different threshold values create trade-offs between precision and recall, allowing practitioners to prioritize either conservative predictions or comprehensive detection based on application needs. The optimal threshold analysis identifies decision boundaries that maximize overall classification performance while considering the costs of different error types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928bd0d7-06f1-4210-aadc-b57c2e918baa",
   "metadata": {},
   "source": [
    "### Adjusting the Threshold Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91d9c082-7908-47eb-9011-b14aaa9e20eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Threshold Performance:\n",
      "Accuracy: 0.8343\n",
      "Precision: 0.6502\n",
      "Recall: 0.5643\n",
      "F1-Score: 0.6042\n"
     ]
    }
   ],
   "source": [
    "# Apply optimal threshold for improved predictions\n",
    "optimal_thresh = 0.4  # Example based on analysis\n",
    "y_pred_optimal = (y_pred_proba >= optimal_thresh).astype(int)\n",
    "\n",
    "# Evaluate performance with adjusted threshold\n",
    "adj_accuracy = accuracy_score(y_test, y_pred_optimal)\n",
    "adj_precision = precision_score(y_test, y_pred_optimal)\n",
    "adj_recall = recall_score(y_test, y_pred_optimal)\n",
    "adj_f1 = f1_score(y_test, y_pred_optimal)\n",
    "\n",
    "print(f\"Adjusted Threshold Performance:\")\n",
    "print(f\"Accuracy: {adj_accuracy:.4f}\")\n",
    "print(f\"Precision: {adj_precision:.4f}\")\n",
    "print(f\"Recall: {adj_recall:.4f}\")\n",
    "print(f\"F1-Score: {adj_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae52f02-9d98-4aa1-9af5-fb7623dc5b50",
   "metadata": {},
   "source": [
    "Threshold adjustment provides fine-grained control over model behavior, enabling optimization for specific operational requirements. Lowering the threshold increases recall at the expense of precision, making the model more sensitive to rainfall detection. This adjustment proves particularly valuable in applications where missing rainfall events carries higher costs than false alarms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af328529-a689-4284-b3af-d28987f1d013",
   "metadata": {},
   "source": [
    "### ROC - AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29004a74-5cf6-4ebb-a551-651d36b68109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.8471\n",
      "Number of threshold points: 8304\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Number of threshold points: {len(fpr)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1889f18-ce1b-44ea-b384-98a1a686e138",
   "metadata": {},
   "source": [
    "The ROC-AUC score of 0.843 indicates excellent discriminative ability between rainfall and non-rainfall conditions. This metric demonstrates that the model can effectively rank-order predictions by rainfall probability, with 84.3% probability that a randomly selected rainy day receives a higher prediction score than a randomly selected non-rainy day. The ROC curve analysis confirms robust performance across various classification thresholds.\n",
    "The area under the ROC curve serves as a threshold-independent measure of classification performance. Values above 0.8 indicate strong predictive capability, while the observed score of 0.843 demonstrates that the logistic regression model successfully captures the complex relationships between weather variables and precipitation outcomes. This performance level supports the model's suitability for operational weather prediction applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e1e4e-4a00-48ad-8b0b-fbdb5626e11d",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a9b3290-f291-4083-ac3f-42a0a0ca3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE Selected Features:\n",
      "1. MaxTemp\n",
      "2. WindGustSpeed\n",
      "3. Humidity3pm\n",
      "4. Pressure3pm\n",
      "5. RainToday\n",
      "\n",
      "Feature Rankings:\n",
      "MinTemp: 5\n",
      "MaxTemp: 1\n",
      "Rainfall: 4\n",
      "WindGustSpeed: 1\n",
      "Humidity9am: 3\n",
      "Humidity3pm: 1\n",
      "Pressure3pm: 1\n",
      "Temp3pm: 2\n",
      "RainToday: 1\n"
     ]
    }
   ],
   "source": [
    "# Apply RFE for feature selection optimization\n",
    "rfe_selector = RFE(estimator=LogisticRegression(max_iter=1000), \n",
    "                   n_features_to_select=5, step=1)\n",
    "rfe_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Identify selected features\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[rfe_selector.support_]\n",
    "feature_rankings = rfe_selector.ranking_\n",
    "\n",
    "print(\"RFE Selected Features:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "\n",
    "print(\"\\nFeature Rankings:\")\n",
    "for feature, rank in zip(feature_names, feature_rankings):\n",
    "    print(f\"{feature}: {rank}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4f497-5258-49c8-b875-7f2c9b78cdaa",
   "metadata": {},
   "source": [
    "- Recursive Feature Elimination systematically identifies the most informative variables for rainfall prediction. This process iteratively removes less important features and retrains the model to determine optimal feature subsets. The technique helps reduce overfitting, improve interpretability, and potentially enhance generalization performance by focusing on the most predictive meteorological variables.\n",
    "- The RFE analysis reveals which weather parameters contribute most significantly to accurate precipitation forecasting. Features consistently selected across iterations demonstrate robust predictive relationships with rainfall occurrence, while eliminated features may represent redundant or noisy variables that could degrade model performance. This feature selection process ensures optimal resource utilization and improved model interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f9cb6-2564-446c-9360-f06420a1517a",
   "metadata": {},
   "source": [
    "### k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d063d698-120c-46af-83cd-1fb720e03dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.83571663 0.83454452 0.8370294  0.84003001 0.83641223]\n",
      "Mean CV Accuracy: 0.8367 (+/- 0.0037)\n",
      "Mean CV Precision: 0.7118\n",
      "Mean CV Recall: 0.4568\n",
      "Mean CV F1-Score: 0.5565\n"
     ]
    }
   ],
   "source": [
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, \n",
    "                           cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Additional scoring metrics\n",
    "cv_precision = cross_val_score(log_reg, X_train_scaled, y_train, \n",
    "                              cv=5, scoring='precision')\n",
    "cv_recall = cross_val_score(log_reg, X_train_scaled, y_train, \n",
    "                           cv=5, scoring='recall')\n",
    "cv_f1 = cross_val_score(log_reg, X_train_scaled, y_train, \n",
    "                        cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Mean CV Precision: {cv_precision.mean():.4f}\")\n",
    "print(f\"Mean CV Recall: {cv_recall.mean():.4f}\")\n",
    "print(f\"Mean CV F1-Score: {cv_f1.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a88d6-33a2-4886-8ac7-e3c9f015c3d0",
   "metadata": {},
   "source": [
    "k-Fold cross-validation provides robust estimation of model performance by evaluating predictions across multiple data partitions. This approach reduces variance in performance estimates and provides more reliable assessment of model generalization capability. The consistent performance across folds indicates stable model behavior and suggests good generalization to unseen weather data.\n",
    "The cross-validation results demonstrate that the model maintains consistent performance across different data subsets, indicating robust learning of weather-rainfall relationships. Standard deviation metrics reveal the stability of performance estimates, with low variance suggesting reliable predictive behavior across diverse weather conditions and geographic locations within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39ff80-9f85-4b46-8269-b881e8d07e39",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization using GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b1e9cad-2964-42d9-a7b5-e83b36762edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best Cross-Validation Score: 0.8454\n",
      "\n",
      "Optimized Model Performance:\n",
      "Test Accuracy: 0.8387\n",
      "Test ROC-AUC: 0.8471\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for optimization\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "best_roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "print(f\"\\nOptimized Model Performance:\")\n",
    "print(f\"Test Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Test ROC-AUC: {best_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2acc7e-e7ba-4ec9-9143-65b16f2f88e4",
   "metadata": {},
   "source": [
    "GridSearchCV systematically explores hyperparameter combinations to identify optimal model configurations. The regularization parameter C controls the strength of regularization, with higher values allowing more complex models while lower values enforce greater simplification. The penalty parameter determines whether L1 or L2 regularization is applied, affecting feature selection behavior and model interpretability.\n",
    "The optimization process reveals that moderate regularization strength typically provides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d5bd5-a65e-42f7-9a26-2d5168797c5e",
   "metadata": {},
   "source": [
    "Visual Logistic Regression Workflow for Rain Prediction\n",
    "\n",
    "This walkthrough demonstrates how to build, verify, and interpret a logistic regression model to predict rainfall using the Australian weather dataset, enhanced with visualizations at each stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10af50-e0ce-49b2-b2e4-8bc3c9e12dcb",
   "metadata": {},
   "source": [
    "1. Data Import and Inspection\n",
    "Begin by loading essential Python packages and your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b3b26c-6b62-47e4-ad5d-36b237d58088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           145460 non-null  object \n",
      " 1   Location       145460 non-null  object \n",
      " 2   MinTemp        143975 non-null  float64\n",
      " 3   MaxTemp        144199 non-null  float64\n",
      " 4   Rainfall       142199 non-null  float64\n",
      " 5   Evaporation    82670 non-null   float64\n",
      " 6   Sunshine       75625 non-null   float64\n",
      " 7   WindGustDir    135134 non-null  object \n",
      " 8   WindGustSpeed  135197 non-null  float64\n",
      " 9   WindDir9am     134894 non-null  object \n",
      " 10  WindDir3pm     141232 non-null  object \n",
      " 11  WindSpeed9am   143693 non-null  float64\n",
      " 12  WindSpeed3pm   142398 non-null  float64\n",
      " 13  Humidity9am    142806 non-null  float64\n",
      " 14  Humidity3pm    140953 non-null  float64\n",
      " 15  Pressure9am    130395 non-null  float64\n",
      " 16  Pressure3pm    130432 non-null  float64\n",
      " 17  Cloud9am       89572 non-null   float64\n",
      " 18  Cloud3pm       86102 non-null   float64\n",
      " 19  Temp9am        143693 non-null  float64\n",
      " 20  Temp3pm        141851 non-null  float64\n",
      " 21  RainToday      142199 non-null  object \n",
      " 22  RainTomorrow   142193 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 25.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('weatherAUS.csv')  # Ensure the dataset is in your directory or provide a valid URL\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12907f-c5af-415d-a43d-0a2ba0603ef1",
   "metadata": {},
   "source": [
    "Comment: This step loads your data into a DataFrame and provides a snapshot and overview to identify missing values, columns, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfdd0a-fbce-4ca0-9a99-24664033abac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
